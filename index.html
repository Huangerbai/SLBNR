
<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
	<title>SLBNR-AAAI2024</title>
    <style type="text/css">
        body{
        	background-color: white;
        }
        .links{
        	text-decoration: none;
        	color: #0066CC;
        }
        .p2{
        	padding-top: 20px;
        	font-size: 25px;
        }
        .p1{
        	text-align:justify;
        	text-justify:inter-ideograph;
        }
		
		.left {
			text-align: left;
			border: 1px dotted black;
			width: 50%;
		}
        a{
        	font-family: Sans-serif;
        }
        p{
        	font-family: Sans-serif;
        }
        ul{
        	font-family: Sans-serif;
        }
    </style>
</head>
<body>
	<div align="center" style="padding-top: 30px;">
	<p style="font-size:35px;">Seeing Dark Videos via Self-Learned Bottleneck Neural Representation </p>

	<a href="mailto:hhf@pku.edu.cn" class="links">Haofeng Huang</a><sup>1</sup> &nbsp; &nbsp; 
	<a href="mailto:yangwh@pcl.ac.cn" class="links">Wenhan Yang</a><sup>2</sup> &nbsp; &nbsp; 
  <a href="mailto:lingyu@pku.edu.cn" class="links">Ling-Yu Duan</a><sup>1</sup> &nbsp; &nbsp; 
	<a href="mailto:liujiaying@pku.edu.cn" class="links">Jiaying Liu</a><sup>1</sup>

	<br>
	<p class="para-3"><span class="p1"> <sup>1</sup> Wangxuan Institute of Computer Technology, Peking University &nbsp; &nbsp; <sup>2</sup> Peng Cheng Laboratory</span></p>

	<p class="para-3"><span class="p1"> Accepted by <i>AAAI 2024.</i></span></p>
	

	</div>

        <div align="left" style="padding-left: 15%; padding-right: 15%; padding-bottom: 30px;">
		<p class='p2'> Abstract </p> 
		<p class='p1'>Enhancing low-light videos in a supervised style presents a set of challenges, including limited data diversity, misalignment, and the domain gap introduced through the dataset construction pipeline. Our paper tackles these challenges by constructing a self-learned enhancement approach that gets rid of the reliance on any external training data. The challenge of self-supervised learning lies in fitting high-quality signal representations solely from input signals. Our work designs a bottleneck neural representation mechanism that extracts those signals. More in detail, we encode the frame-wise representation with a compact deep embedding and utilize a neural network to parameterize the video-level manifold consistently. Then, an entropy constraint is applied to the enhanced results based on the adjacent spatial-temporal context to filter out the degraded visual signals, e.g. noise and frame inconsistency. Last, a novel Chromatic Retinex decomposition is proposed to effectively align the reflectance distribution temporally. It benefits the entropy control on different components of each frame and facilitates noise-to-noise training, successfully suppressing the temporal flicker. Extensive experiments demonstrate the robustness and superior effectiveness of our proposed method.</p>
            </div>
			
        <div align="left" style="padding-left: 15%; padding-right: 15%; padding-bottom: 30px;">
            <p class='p2'> Method </p> 
			<p style="line-height:180%">
			<strong>Key Idea</strong>: 
			<br>
			(1) <i> Objective bottleneck</i>. We adopt an entropy control mechanism as the objective bottleneck. Without any explicit alignment, it implicitly fuses spatial-temporal information to provide reliable guidance of the center pixel.
			<br>
			(2) <i> Content bottleneck</i>. We adopt neural representation as the content bottleneck, which utilizes the inductive bias of the neural network to predict the noise-free signal, getting rid of any assumption on the noise model.
			<br>
			(3) <i> Chromatic Retinex</i>. We adjust the formulation of the Retinex model into a chromatic illumination form that facilitates suppressing the color bias artifacts.
			</p>
			
			<br>
            <div style="padding-left: 5%; padding-right: 5%;">
                <div align="center">
                    <img src="img/framework.png" width="100%"> <br>
                </div>
            <p style="line-height:180%">Figure 1. The framework of the proposed bottleneck neural representation. 
    A constrained deep embedding is first extracted and then transformed into enhanced Retinex-based layer-wise representations.
    Hybrid neural representation provides richer intrinsic information but still set bottlenecks from the perspective of content.
    Entropy minimization applies the bottleneck constraint in the objective view to suppress noise and correct illumination.
    A chromatic Retinex representation helps align layer-wise frames, which facilitates self-supervised learning.
            
			</div>
        
            <p class='p2'> Results </p> 
            <div style="padding-left: 5%; padding-right: 5%;">
                <div align="center">
					<p style="line-height:150%">
					Table 1. Subject results on DRV-dynamic datasets.
					</p>
                    <img src="img/score.png" width="100%"> <br>
                </div>
			<br>

                <div align="center">
					<p style="line-height:150%">
					Table 2. Objective results on DRV-dynamic dataset.
					</p>
                    <img src="img/result.png" width="50%"> <br>
                </div>
			</div>
		
	<!--
	<p class='p2'> Resources </p> 
	<p class='p1'>
		<ul style="line-height:15px">
		　　<li> Paper: <a href="https://arxiv.org/abs/2211.13466" class="links">arXiv</a> </li>
		　　<li> <a href="https://github.com/JHang2020/HiCLR" class="links">Code</a> </li>
		</ul>
	</p>
	-->

	<p class='p2'> Citation</p>
	<p> 
		@article{huang2024seeing, <br>
			&nbsp; &nbsp; title={Seeing Dark Videos via Self-Learned Bottleneck Neural Representation},<br>
			&nbsp; &nbsp; author={Huang, Haofeng and Yang, Wenhan and Duan, Ling-Yu and Liu, Jiaying},<br>
			&nbsp; &nbsp; booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
			&nbsp; &nbsp; year={2024}, <br>
			}
	</p>
	<!--
	<p class='p2'> Reference</p>
	<p> 
		[1] Liu, J.; Song, S.; Liu, C.; Li, Y.; and Hu, Y. A benchmark dataset and comparison study for multi-modal human action analytics. <i>TOMM</i> 2020. <br> <br>
		[2] Shahroudy, A.; Liu, J.; Ng, T.-T.; and Wang, G. Ntu rgb+ d: A large scale dataset for 3d human activity analysis. <i>CVPR</i> 2016. <br> <br>
		[3] Liu, J.; Shahroudy, A.; Perez, M.; Wang, G.; Duan, L.-Y.; and Kot, A. C. NTU RGB + D 120: A large-scale benchmark for 3D human activity understanding. <i>TPAMI</i> 2019. <br> <br>
		[4] Guo, T.; Liu, H.; Chen, Z.; Liu, M.; Wang, T.; and Ding, R. Contrastive Learning from Extremely Augmented Skeleton Sequences for Self-supervised Action Recognition. <i>AAAI</i> 2022. <br> <br>
		[5] Lin, L.; Song, S.; Yang, W.; and Liu, J. MS2L: Multi-task self-supervised learning for skeleton based action recognition. <i>ACM MM</i> 2020.
	</p>
	-->
	
	<p class='left'>
		<ul style="line-height:15px">
			Return to the <a href="http://39.96.165.147/Projects.html" class="links">STRUCT Project</a>
		</ul>
	</p>
		

</html>
